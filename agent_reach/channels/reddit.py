# -*- coding: utf-8 -*-
"""Reddit â€” via Reddit JSON API + optional proxy.

Backend: Reddit public JSON API (append .json to any URL)
Swap to: any Reddit access method
"""

import json
import os
import requests
from datetime import datetime, timezone
from urllib.parse import urlparse
from .base import Channel, ReadResult


class RedditChannel(Channel):
    name = "reddit"
    description = "Reddit å¸–å­å’Œè¯„è®º"
    backends = ["Reddit JSON API"]
    tier = 2

    USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"

    def can_handle(self, url: str) -> bool:
        domain = urlparse(url).netloc.lower()
        return "reddit.com" in domain or "redd.it" in domain

    def check(self, config=None):
        proxy = config.get("reddit_proxy") if config else None
        has_bot = bool(os.environ.get("REDDIT_CLIENT_ID"))
        if proxy and has_bot:
            return "ok", "å®Œæ•´å¯ç”¨ï¼ˆä»£ç† + OAuth Botï¼‰"
        elif proxy:
            return "ok", "ä»£ç†å·²é…ç½®ï¼Œå¯è¯»å–å¸–å­ã€‚é…ç½® REDDIT_CLIENT_ID/SECRET å¯è§£é”é«˜çº§æœç´¢å’Œå‘å¸–"
        elif has_bot:
            return "warn", "OAuth Bot å·²é…ç½®ï¼Œä½†æœåŠ¡å™¨ç›´è¿å¯èƒ½è¢«å°ã€‚é…ä¸ªä»£ç†æ›´ç¨³å®šï¼šagent-reach configure proxy URL"
        else:
            return "off", "æœç´¢ç”¨ Exa å…è´¹å¯ç”¨ã€‚è¯»å¸–å­éœ€é…ä¸ªä»£ç†ï¼šagent-reach configure proxy URL"

    async def read(self, url: str, config=None) -> ReadResult:
        proxy = config.get("reddit_proxy") if config else None
        proxies = {"http": proxy, "https": proxy} if proxy else None

        parsed = urlparse(url)

        # Fix #1: Resolve redd.it short links via HEAD redirect
        if "redd.it" in parsed.netloc:
            try:
                head_resp = requests.head(
                    url,
                    allow_redirects=True,
                    timeout=10,
                    headers={"User-Agent": self.USER_AGENT},
                    proxies=proxies,
                )
                url = head_resp.url
                parsed = urlparse(url)
            except Exception:
                pass  # Fall through with original URL; may 404

        # Clean URL: remove query params, trailing slash, then add .json
        clean_path = parsed.path.rstrip("/")
        # Remove trailing .json if already present (avoid double .json)
        if clean_path.endswith(".json"):
            clean_path = clean_path[:-5]
        json_url = f"https://www.reddit.com{clean_path}.json"

        # Fix #5: Catch network-level exceptions
        try:
            resp = requests.get(
                json_url,
                headers={"User-Agent": self.USER_AGENT},
                proxies=proxies,
                params={"limit": 50},
                timeout=15,
            )
        except requests.exceptions.ConnectionError:
            return ReadResult(
                title="Reddit",
                content="âš ï¸ æ— æ³•è¿æ¥åˆ° Redditï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†é…ç½®ã€‚",
                url=url,
                platform="reddit",
            )
        except requests.exceptions.Timeout:
            return ReadResult(
                title="Reddit",
                content="âš ï¸ è¯·æ±‚ Reddit è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥ä»£ç†ã€‚",
                url=url,
                platform="reddit",
            )
        except requests.exceptions.ProxyError:
            return ReadResult(
                title="Reddit",
                content="âš ï¸ ä»£ç†è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç†åœ°å€å’Œå‡­è¯ã€‚",
                url=url,
                platform="reddit",
            )
        except requests.exceptions.RequestException as e:
            return ReadResult(
                title="Reddit",
                content=f"âš ï¸ è¯·æ±‚å¤±è´¥: {e}",
                url=url,
                platform="reddit",
            )

        # Fix #4: Friendly errors for all HTTP status codes
        if resp.status_code != 200:
            if resp.status_code in (403, 429):
                return ReadResult(
                    title="Reddit",
                    content="âš ï¸ Reddit blocked this request (403 Forbidden). "
                            "Reddit blocks most server IPs.\n"
                            "Fix: agent-reach configure proxy http://user:pass@ip:port\n"
                            "Cheap option: https://www.webshare.io ($1/month)\n\n"
                            "Alternatively, search Reddit via Exa (free, no proxy needed): "
                            "agent-reach search-reddit \"your query\"",
                    url=url,
                    platform="reddit",
                )
            return ReadResult(
                title="Reddit",
                content=f"âš ï¸ Reddit è¿”å› HTTP {resp.status_code}: {url}",
                url=url,
                platform="reddit",
            )

        # Fix #2: Safe JSON parsing
        try:
            data = resp.json()
        except (json.JSONDecodeError, ValueError):
            return ReadResult(
                title="Reddit",
                content=f"âš ï¸ Reddit è¿”å›äº†é JSON å“åº”: {url}",
                url=url,
                platform="reddit",
            )

        # Subreddit listing page: /r/sub/, /r/sub/hot, /r/sub/new, /r/sub/top
        if isinstance(data, dict) and data.get("kind") == "Listing":
            return self._parse_listing(data, url)

        if isinstance(data, list) and len(data) >= 1:
            # Fix #2 continued: Safe nested access
            children = data[0].get("data", {}).get("children", [])
            if not children:
                return ReadResult(
                    title="Reddit",
                    content=f"âš ï¸ å¸–å­ä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤: {url}",
                    url=url,
                    platform="reddit",
                )

            post = children[0].get("data", {})
            title = post.get("title", "")
            author = post.get("author", "")
            selftext = post.get("selftext", "")
            score = post.get("score", 0)
            subreddit = post.get("subreddit", "")

            # Fix #3: Extract created_utc timestamp
            created = post.get("created_utc", 0)
            date_str = (
                datetime.fromtimestamp(created, tz=timezone.utc).strftime("%Y-%m-%d %H:%M UTC")
                if created
                else ""
            )

            # Extract comments
            comments_text = ""
            if len(data) >= 2:
                comments_text = self._extract_comments(data[1])

            content = selftext
            if comments_text:
                content += f"\n\n---\n## Comments\n{comments_text}"

            return ReadResult(
                title=title,
                content=content,
                url=url,
                author=f"u/{author}",
                platform="reddit",
                date=date_str,
                extra={"subreddit": subreddit, "score": score},
            )

        return ReadResult(
            title="Reddit",
            content=f"âš ï¸ Reddit è¿”å›äº†æ„å¤–æ ¼å¼: {url}",
            url=url,
            platform="reddit",
        )

    def _parse_listing(self, data: dict, url: str) -> ReadResult:
        """Parse a subreddit listing (hot/new/top/rising)."""
        children = data.get("data", {}).get("children", [])

        # Extract subreddit name and sort from URL
        parsed = urlparse(url)
        path_parts = [p for p in parsed.path.strip("/").split("/") if p]
        subreddit = path_parts[1] if len(path_parts) >= 2 else "reddit"
        sort_type = path_parts[2] if len(path_parts) >= 3 else "hot"

        lines = []
        for i, child in enumerate(children, 1):
            if child.get("kind") != "t3":
                continue
            post = child.get("data", {})
            title = post.get("title", "")
            author = post.get("author", "")
            score = post.get("score", 0)
            num_comments = post.get("num_comments", 0)
            permalink = post.get("permalink", "")
            post_url = post.get("url", "")
            is_self = post.get("is_self", False)

            # Fix #3: timestamp in listing items
            created = post.get("created_utc", 0)
            date_str = (
                datetime.fromtimestamp(created, tz=timezone.utc).strftime("%Y-%m-%d %H:%M UTC")
                if created
                else ""
            )

            lines.append(f"### {i}. {title}")
            date_part = f" Â· ğŸ“… {date_str}" if date_str else ""
            lines.append(f"ğŸ‘¤ u/{author} Â· â¬† {score} Â· ğŸ’¬ {num_comments}{date_part}")
            if not is_self and post_url:
                lines.append(f"ğŸ”— {post_url}")
            lines.append(f"ğŸ“ https://www.reddit.com{permalink}")
            # Add selftext preview (first 200 chars)
            selftext = post.get("selftext", "")
            if selftext:
                preview = selftext[:200].replace("\n", " ")
                if len(selftext) > 200:
                    preview += "..."
                lines.append(f"> {preview}")
            lines.append("")

        content = "\n".join(lines) if lines else "No posts found."
        return ReadResult(
            title=f"r/{subreddit} â€” {sort_type}",
            content=content,
            url=url,
            platform="reddit",
            extra={"subreddit": subreddit, "sort": sort_type, "count": len(children)},
        )

    def _extract_comments(self, comments_data: dict, depth: int = 0, max_depth: int = 3) -> str:
        """Recursively extract comments."""
        lines = []
        children = comments_data.get("data", {}).get("children", [])

        for child in children:
            kind = child.get("kind")

            # Fix #6: Show "more" comments hint instead of silently skipping
            if kind == "more":
                count = child.get("data", {}).get("count", 0)
                if count > 0:
                    indent = "  " * depth
                    lines.append(f"{indent}*[è¿˜æœ‰ {count} æ¡è¯„è®ºæœªåŠ è½½]*")
                    lines.append("")
                continue

            if kind != "t1":
                continue

            data = child.get("data", {})
            author = data.get("author", "[deleted]")
            body = data.get("body", "")
            score = data.get("score", 0)
            indent = "  " * depth

            lines.append(f"{indent}**u/{author}** ({score} points):")
            lines.append(f"{indent}{body}")
            lines.append("")

            # Recurse into replies
            if depth < max_depth and data.get("replies") and isinstance(data["replies"], dict):
                lines.append(self._extract_comments(data["replies"], depth + 1, max_depth))

        return "\n".join(lines)
